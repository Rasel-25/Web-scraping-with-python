{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=requests.get(r'https://quotes.toscrape.com')\n",
    "res.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bs4 — BeautifulSoup 4\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files\n",
    "Beautiful Soup is a library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "It commonly saves programmers hours or days of work.\n",
    "\n",
    "Beautiful Soup is a library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lxml\n",
    "lxml is a Python library which allows for easy handling of XML and HTML files, and can also be used for web scraping. There are a lot of off-the-shelf XML parsers out there, but for better results, developers sometimes prefer to write their own XML and HTML parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be the name of a specific parser (\"lxml\", \"lxml-xml\", \"html.parser\", or \"html5lib\") or it may be the type of markup to be used (\"html\", \"html5\", \"xml\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(res.content,'lxml') # \n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title # getting title with tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.head  # getting head of the HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body   # getting body of the HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.text # getting only title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('title') # return single tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('title').string # text / string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('head') # return multiple tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<link href=\"/static/bootstrap.min.css\" rel=\"stylesheet\"/>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.link # return single link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<link href=\"/static/bootstrap.min.css\" rel=\"stylesheet\"/>,\n",
       " <link href=\"/static/main.css\" rel=\"stylesheet\"/>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('link') # return multiple link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rel': ['stylesheet'], 'href': '/static/bootstrap.min.css'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.link.attrs # return link as list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/static/bootstrap.min.css'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.link.attrs['href'] # return only link text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div', attrs={'class':'quote'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the quote\n",
    "soup.find('div',attrs={'class':'quote'}).span.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the author\n",
    "soup.find('div',attrs={'class':'quote'}).find('small',attrs={'class':'author'}).string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the tags\n",
    "tags=soup.find('div',attrs={'class':'quote'}).find('div',attrs={'class':'tags'}).find_all('a')\n",
    "tags\n",
    "for tag in tags:\n",
    "    print(tag.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Python for loop one line with list comprehension\n",
    "   \n",
    "   Syntax \n",
    "   new_list = [expression for member in iterable]\n",
    " \n",
    "    my_list = [n*n for n in range(1, 10)]\n",
    "    print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all iterable string \n",
    "database=[]\n",
    "for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "    q=quote.span.string\n",
    "    \n",
    "    database.append(q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all iterable author \n",
    "database=[]\n",
    "for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "    q=quote.span.string\n",
    "    author=quote.find('small',attrs={'class':'author'}).string\n",
    "    database.append(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all iterable tags\n",
    "#  \n",
    "database=[]\n",
    "for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "    q=quote.span.string\n",
    "    #author=quote.find('small',attrs={'class':'author'}).string\n",
    "    tags=[tag.string for tag in quote.find('div',attrs={'class':'tags'}).find_all('a') ]\n",
    "    database.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all iterable  \n",
    "database=[]\n",
    "for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "    q=quote.span.string\n",
    "    author=quote.find('small',attrs={'class':'author'}).string\n",
    "    tags=[tag.string for tag in quote.find('div',attrs={'class':'tags'}).find_all('a') ]\n",
    "    database.append([q,author,tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('li',attrs={'class':'next'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find('li',class_='next').a.attrs['href'] # class_='next' smilat to attrs={'class':'next'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPage(soup:BeautifulSoup):\n",
    "    tempData=[]\n",
    "    for quote in soup.find_all('div',attrs={'class': 'quote'}):\n",
    "        q=quote.span.string\n",
    "        author=quote.find('small',attrs={'class':'author'}).string\n",
    "        tags='#'.join([tag.string for tag in quote.find('div',attrs={'class':'tags'}).find_all('a')])\n",
    "        tempData.append([q,author,tags])\n",
    "    return tempData\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url=r'https://quotes.toscrape.com'\n",
    "resp=requests.get(start_url)\n",
    "#resp.content\n",
    "Database = []\n",
    "while True:\n",
    "    try:\n",
    "        soup=BeautifulSoup(resp.content)\n",
    "        extractedQuote=extractPage(soup)\n",
    "        Database = Database + extractedQuote # store all data into database\n",
    "        neext=soup.find('li',attrs={'class':'next'}).a.attrs['href'] # return '/page/10/'\n",
    "        url=start_url+neext # return all url\n",
    "        print(url)\n",
    "        resp = requests.get(url)\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print('Successfully all pages Extracted!')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Database[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv.writer\n",
    "There are various classes provided by this module for writing to CSV:\n",
    "\n",
    "1) Using csv.writer class\n",
    "2) Using csv.DictWriter class\n",
    "\n",
    "Using csv.writer class\n",
    "csv.writer class is used to insert data to the CSV file. This class returns a writer object which is responsible for converting the user’s data into a delimited string. A csvfile object should be opened with newline='' otherwise newline characters inside the quoted fields will not be interpreted correctly.\n",
    "\n",
    "Syntax: csv.writer(csvfile, dialect=’excel’, **fmtparams)\n",
    "\n",
    "Parameters:\n",
    "csvfile: A file object with write() method.\n",
    "dialect (optional): Name of the dialect to be used.\n",
    "fmtparams (optional): Formatting parameters that will overwrite those specified in the dialect.\n",
    "\n",
    "csv.writer class provides two methods for writing to CSV. They are writerow() and writerows().\n",
    "\n",
    "writerow(): This method writes a single row at a time. Field row can be written using this method.\n",
    "Syntax:\n",
    "writerow(fields)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode()\n",
    "The encode() method encodes the string, using the specified encoding. If no encoding is specified, UTF-8 will be used.\n",
    "\n",
    "A String specifying the encoding to use. Default is UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# writing to csv file \n",
    "with open('Quotes.csv', mode='w', newline='', encoding='utf-8') as f: # newline='' add new line\n",
    "    \n",
    "    writer = csv.writer(f, delimiter='|')  # creating a csv writer object \n",
    "    writer.writerow(['Quote', 'Author', 'Tags'])   # writing the fields/heading name. \n",
    "    writer.writerows(Database)    # writing the data rows "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
